{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical Questions\n",
        "\n",
        "\n",
        "1. Can we use Bagging for regression problems?\n",
        "\n",
        "  -> Yes, Bagging can be used for regression problems. It improves the stability and accuracy of regression models by averaging predictions from multiple models trained on different bootstrap samples of the data. For example, a Bagging Regressor can be applied to predict housing prices by averaging the outputs of several decision trees trained on different subsets of the dataset​.\n",
        "\n",
        "2.What is the difference between multiple model training and single model training?\n",
        "\n",
        "  -> Multiple model training involves training several models independently and combining their predictions, while single model training focuses on optimizing one model. For instance, in ensemble methods like Bagging, multiple decision trees are trained separately and their predictions are averaged, whereas a single decision tree would be trained on the entire dataset without aggregation​.\n",
        "\n",
        "3. Explain the concept of feature randomness in Random Forest.\n",
        "\n",
        "  -> Feature randomness in Random Forest refers to the practice of selecting a random subset of features for each decision tree during training. This reduces correlation among trees and enhances model diversity. For example, if a dataset has 10 features, a tree might only use 3 randomly selected features to make splits, leading to more robust predictions​.\n",
        "\n",
        "4. What is OOB (Out-of-Bag) Score?\n",
        "\n",
        "  ->  The OOB Score is an internal validation method used in Random Forests to estimate the model's performance. It uses the data points that were not included in the bootstrap samples for each tree to evaluate the model. This provides an unbiased estimate of the model's accuracy without needing a separate validation set​.\n",
        "\n",
        "5. How can you measure the importance of features in a Random Forest model?\n",
        "\n",
        "  ->  Feature importance in a Random Forest model can be measured using two main methods: Mean Decrease Impurity (MDI) and Mean Decrease Accuracy (MDA). MDI calculates the total decrease in node impurity (e.g., Gini impurity) brought by a feature, while MDA assesses the drop in model accuracy when the feature's values are permuted. Features with higher importance scores are more influential in making predictions​.\n",
        "\n",
        "6. Explain the working principle of a Bagging Classifier.\n",
        "\n",
        "  -> A Bagging Classifier works by creating multiple bootstrap samples from the training dataset, training a separate classifier (e.g., decision tree) on each sample, and then aggregating their predictions through majority voting for classification tasks. This process reduces variance and improves overall model accuracy. For example, if three classifiers predict classes A, A, and B, the final prediction will be A​.\n",
        "\n",
        "7. How do you evaluate a Bagging Classifier’s performance?\n",
        "\n",
        "  -> The performance of a Bagging Classifier can be evaluated using metrics such as accuracy, precision, recall, and F1-score. Additionally, the Out-of-Bag (OOB) error can be used as a validation score, providing an estimate of the model's generalization performance without needing a separate test set​.\n",
        "\n",
        "8. How does a Bagging Regressor work?\n",
        "\n",
        "  ->  A Bagging Regressor operates similarly to a Bagging Classifier but focuses on regression tasks. It generates multiple bootstrap samples, trains a regressor (e.g., decision tree) on each sample, and averages the predictions from all regressors to produce the final output. This helps to reduce overfitting and improve prediction accuracy. For instance, predicting house prices by averaging the outputs of several regression trees trained on different subsets of the data​.\n",
        "\n",
        "9. What is the main advantage of ensemble techniques?\n",
        "\n",
        "  -> The main advantage of ensemble techniques is their ability to improve predictive performance by combining multiple models, which reduces the risk of overfitting and increases robustness. For example, Random Forests, an ensemble of decision trees, often outperform individual trees by leveraging the diversity of multiple models to achieve better accuracy​.\n",
        "\n",
        "10. What is the main challenge of ensemble methods?\n",
        "\n",
        "   ->  The main challenge of ensemble methods is their increased computational complexity and resource requirements, as they involve training multiple models. This can lead to longer training times and higher memory usage, especially with large datasets or complex models. For instance, training a Random Forest with hundreds of trees can be computationally intensive compared to a single decision tree​.\n",
        "\n",
        "11. Explain the key idea behind ensemble techniques.\n",
        "\n",
        "   -> The key idea behind ensemble techniques is to combine the predictions of multiple models to improve overall performance. By aggregating diverse models, ensembles can reduce errors and enhance generalization. For example, in Bagging, multiple decision trees are trained on different subsets of data, and their predictions are averaged to create a more accurate final prediction​.\n",
        "\n",
        "12. What is a Random Forest Classifier?\n",
        "\n",
        "  ->  A Random Forest Classifier is an ensemble learning method that constructs a multitude of decision trees during training and outputs the mode of their predictions for classification tasks. It uses bootstrap sampling and feature randomness to create diverse trees, which helps improve accuracy and reduce overfitting. For example, it can classify whether an email is spam or not based on various features like keywords and sender information​.\n",
        "\n",
        "13.What are the main types of ensemble techniques?\n",
        "\n",
        "  ->  The main types of ensemble techniques include:\n",
        "    Bagging: Combines predictions from multiple models trained on different bootstrap samples (e.g., Random Forest).\n",
        "    Boosting: Sequentially trains models, where each new model focuses on correcting errors made by previous ones (e.g., AdaBoost).\n",
        "    Stacking: Trains multiple models and then uses another model to combine their predictions (e.g., using logistic regression to combine outputs from decision trees and SVMs)​.\n",
        "\n",
        "14. What is ensemble learning in machine learning?\n",
        "\n",
        " ->   Ensemble learning is a machine learning paradigm that combines multiple models to improve overall performance and robustness. By aggregating the predictions of various models, ensemble methods can reduce variance, bias, and improve accuracy. For example, using a combination of decision trees and logistic regression can yield better results than using either model alone​.\n",
        "\n",
        "15. When should we avoid using ensemble methods?\n",
        "\n",
        "   -> Ensemble methods should be avoided when the dataset is small or when the individual models are already highly accurate and stable. In such cases, the added complexity and computational cost of ensembles may not provide significant benefits. For instance, using an ensemble approach on a small dataset may lead to overfitting rather than improved generalization​.\n",
        "\n",
        "16. How does Bagging help in reducing overfitting?\n",
        "\n",
        "  ->  Bagging helps reduce overfitting by averaging the predictions of multiple models trained on different subsets of the data. This averaging process smooths out the predictions and mitigates the impact of noise and outliers, leading to a more generalized model. For example, in a Bagging Regressor, the predictions from several decision trees are averaged, which reduces the variance associated with individual trees and improves overall accuracy​.\n",
        "\n",
        "17. Why is Random Forest better than a single Decision Tree?\n",
        "\n",
        "  -> Random Forest is better than a single Decision Tree because it reduces overfitting and improves accuracy by aggregating the predictions of multiple trees. While a single tree can be highly sensitive to noise and outliers, a Random Forest averages the results from many trees, leading to more stable and reliable predictions. For example, in predicting customer churn, a Random Forest can provide more accurate results by considering various decision paths from multiple trees​.\n",
        "\n",
        "18. What is the role of bootstrap sampling in Bagging?\n",
        "\n",
        "  ->  Bootstrap sampling in Bagging involves creating multiple subsets of the training data by sampling with replacement. Each subset is used to train a separate model, and the predictions from these models are aggregated to form the final prediction. This process helps to reduce variance and improve model stability. For instance, if a dataset has 100 samples, bootstrap sampling might create several subsets, each containing around 67 samples, with some samples appearing multiple times and others not at all​.\n",
        "\n",
        "19. What are some real-world applications of ensemble techniques?\n",
        "\n",
        "  -> Real-world applications of ensemble techniques include:\n",
        "   Finance: Credit scoring models use ensembles to predict loan defaults.\n",
        "   Healthcare: Disease diagnosis models combine predictions from various algorithms to improve accuracy.\n",
        "  Marketing: Customer segmentation models use ensembles to identify target audiences based on purchasing behavior​.\n",
        "\n",
        "20. What is the difference between Bagging and Boosting?\n",
        "\n",
        "   ->  The difference between Bagging and Boosting lies in their approach to model training. Bagging trains multiple models independently on different subsets of the data and combines their predictions, while Boosting trains models sequentially, where each new model focuses on correcting the errors of the previous ones. For example, Bagging might use random forests, while Boosting might use AdaBoost to improve weak learners by emphasizing misclassified instances in subsequent iterations​."
      ],
      "metadata": {
        "id": "PDtFiMwriMk6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical Questions"
      ],
      "metadata": {
        "id": "_8WSKiENj5cJ"
      }
    },
    {
      "source": [
        "#1. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Bagging Classifier with Decision Trees\n",
        "\n",
        "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "zqD1WOX-l6_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.datasets import make_regression\n",
        "\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Bagging Regressor with Decision Trees\n",
        "bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_regressor.predict(X_test)\n",
        "\n",
        "# Calculate Mean Squared Error\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f\"Bagging Regressor MSE: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "n_ZKEOwHmAD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#3. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X = breast_cancer.data\n",
        "y = breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Print feature importance scores\n",
        "for i, feature in enumerate(breast_cancer.feature_names):\n",
        "    print(f\"{feature}: {feature_importances[i]:.4f}\")"
      ],
      "metadata": {
        "id": "BzcOsynwmfx3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#4. Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# Import RandomForestRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor # This line is added to import the necessary class\n",
        "# Create a sample regression dataset\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train a Decision Tree Regressor\n",
        "dt_regressor = DecisionTreeRegressor(random_state=42)\n",
        "dt_regressor.fit(X_train, y_train)\n",
        "y_pred_dt = dt_regressor.predict(X_test)\n",
        "mse_dt = mean_squared_error(y_test, y_pred_dt)\n",
        "\n",
        "# Train a Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "y_pred_rf = rf_regressor.predict(X_test)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# Compare MSE\n",
        "print(f\"Decision Tree Regressor MSE: {mse_dt:.4f}\")\n",
        "print(f\"Random Forest Regressor MSE: {mse_rf:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ecv1Ar8znJIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#5. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
        "# Import RandomForestRegressor for regression tasks\n",
        "from sklearn.ensemble import RandomForestRegressor # Import the appropriate class for regression\n",
        "\n",
        "# Initialize the Random Forest Regressor with OOB score enabled\n",
        "rf_regressor_oob = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=42) # Use RandomForestRegressor for continuous targets\n",
        "\n",
        "# Train the model\n",
        "rf_regressor_oob.fit(X_train, y_train)\n",
        "\n",
        "# Print the OOB score\n",
        "print(f\"OOB Score: {rf_regressor_oob.oob_score_:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "caEuBXCRnwN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#6. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "# Assuming you want to use the breast cancer dataset for this classification task\n",
        "from sklearn.datasets import load_breast_cancer # Import the dataset\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "breast_cancer = load_breast_cancer()\n",
        "X, y = breast_cancer.data, breast_cancer.target\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize the Bagging Classifier with SVM\n",
        "bagging_svm_classifier = BaggingClassifier(estimator=SVC(probability=True), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_svm = bagging_svm_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
        "print(f\"Bagging Classifier with SVM Accuracy: {accuracy_svm:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "ZB_CoaproMHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
        "# Initialize lists to store accuracy results\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "accuracies = []\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=n, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Random Forest with {n} trees Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "92mS62EgoVUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#8. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Initialize the Bagging Classifier with Logistic Regression\n",
        "bagging_lr_classifier = BaggingClassifier(estimator=LogisticRegression(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_lr = bagging_lr_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate AUC score\n",
        "auc_score = roc_auc_score(y_test, y_pred_lr)\n",
        "print(f\"Bagging Classifier with Logistic Regression AUC Score: {auc_score:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "F3lh4KNEoreV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#9. Train a Random Forest Regressor and analyze feature importance scores\n",
        "# Initialize the Random Forest Regressor\n",
        "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "rf_regressor.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importance scores\n",
        "feature_importances_regressor = rf_regressor.feature_importances_\n",
        "\n",
        "# Print feature importance scores\n",
        "for i, feature in enumerate(breast_cancer.feature_names):\n",
        "    print(f\"{feature}: {feature_importances_regressor[i]:.4f}\")"
      ],
      "metadata": {
        "id": "MWoIVJmUoz9X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "source": [
        "#10. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
        "# Initialize the Bagging Classifier with Decision Trees\n",
        "# Change 'base_estimator' to 'estimator'\n",
        "bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the Bagging Classifier\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_bagging = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
        "\n",
        "# Train a Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy for Random Forest\n",
        "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
        "\n",
        "# Compare accuracies\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n",
        "print(f\"Random Forest Classifier Accuracy: {accuracy_rf:.4f}\")"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "io1urcUppNDZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#11. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define the parameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10]\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and accuracy\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")"
      ],
      "metadata": {
        "id": "QKDE6nb_pQub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#12. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
        "# Initialize lists to store MSE results\n",
        "n_estimators_list = [10, 50, 100, 200]\n",
        "mse_results = []\n",
        "\n",
        "for n in n_estimators_list:\n",
        "    bagging_regressor = BaggingRegressor(DecisionTreeRegressor(), n_estimators=n, random_state=42)\n",
        "    bagging_regressor.fit(X_train, y_train)\n",
        "    y_pred = bagging_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_results.append(mse)\n",
        "    print(f\"Bagging Regressor with {n} estimators MSE: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "q5XuJ2qxpl9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#13. Train a Random Forest Classifier and analyze misclassified samples\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Identify misclassified samples\n",
        "misclassified_samples = X_test[y_pred_rf != y_test]\n",
        "print(f\"Number of Misclassified Samples: {len(misclassified_samples)}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "RR6vUCNtpxI8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#14. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n",
        "# Train a single Decision Tree Classifier\n",
        "single_tree_classifier = DecisionTreeClassifier(random_state=42)\n",
        "single_tree_classifier.fit(X_train, y_train)\n",
        "y_pred_single_tree = single_tree_classifier.predict(X_test)\n",
        "accuracy_single_tree = accuracy_score(y_test, y_pred_single_tree)\n",
        "\n",
        "# Compare with Bagging Classifier\n",
        "print(f\"Single Decision Tree Classifier Accuracy: {accuracy_single_tree:.4f}\")\n",
        "print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")"
      ],
      "metadata": {
        "id": "aHWa2u9Pp5ny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#15. Train a Random Forest Classifier and visualize the confusion matrix\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_rf = rf_classifier.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred_rf)\n",
        "\n",
        "# Visualize confusion matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Malignant', 'Benign'], yticklabels=['Malignant', 'Benign'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix for Random Forest Classifier')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ROe78KT3p--P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#16. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "# Define the base models\n",
        "base_models = [\n",
        "    ('dt', DecisionTreeClassifier()),\n",
        "    ('svm', SVC(probability=True)),\n",
        "    ('lr', LogisticRegression())\n",
        "]\n",
        "\n",
        "# Initialize the Stacking Classifier\n",
        "stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_stacking = stacking_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_stacking:.4f}\")"
      ],
      "metadata": {
        "id": "2_J48Rr3qYBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#17. Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_scores = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Random Forest Classifier')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qe6E0wbRqeej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#18. Train a Random Forest Classifier and print the top 5 most important features\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "\n",
        "# Load the breast cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize and train the Random Forest Classifier\n",
        "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf_classifier.feature_importances_\n",
        "\n",
        "# Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values('Importance', ascending=False)\n",
        "\n",
        "# Print the top 5 most important features\n",
        "top_5_features = feature_importance_df.head(5)\n",
        "print(\"Top 5 Most Important Features:\")\n",
        "print(top_5_features)"
      ],
      "metadata": {
        "id": "JRdJ9bg5qtaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#19. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Initialize the Bagging Classifier with Decision Trees\n",
        "bagging_classifier = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train the model\n",
        "bagging_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = bagging_classifier.predict(X_test)\n",
        "\n",
        "# Calculate Precision, Recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")"
      ],
      "metadata": {
        "id": "MJ04uJWEreFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#20. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n",
        "max_depths = [None, 5, 10, 15, 20]\n",
        "accuracies = []\n",
        "\n",
        "for max_depth in max_depths:\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n",
        "    rf_classifier.fit(X_train, y_train)\n",
        "    accuracy = rf_classifier.score(X_test, y_test)\n",
        "    accuracies.append(accuracy)\n",
        "    print(f\"Random Forest with max_depth={max_depth} Accuracy: {accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "YRwJB5vQrmek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#21. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n",
        "from sklearn.ensemble import BaggingRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Create a sample regression dataset\n",
        "from sklearn.datasets import make_regression\n",
        "X_reg, y_reg = make_regression(n_samples=100, n_features=1, noise=0.1, random_state=42)\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train_reg, X_test_reg, y_train_reg, y_test_reg = train_test_split(X_reg, y_reg, test_size=0.3, random_state=42)\n",
        "\n",
        "# Initialize Bagging Regressors\n",
        "bagging_dt = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50, random_state=42)\n",
        "bagging_knn = BaggingRegressor(KNeighborsRegressor(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Train and evaluate Decision Tree Regressor\n",
        "bagging_dt.fit(X_train_reg, y_train_reg)\n",
        "y_pred_dt = bagging_dt.predict(X_test_reg)\n",
        "mse_dt = mean_squared_error(y_test_reg, y_pred_dt)\n",
        "\n",
        "# Train and evaluate KNeighbors Regressor\n",
        "bagging_knn.fit(X_train_reg, y_train_reg)\n",
        "y_pred_knn = bagging_knn.predict(X_test_reg)\n",
        "mse_knn = mean_squared_error(y_test_reg, y_pred_knn)\n",
        "\n",
        "print(f\"Bagging Regressor (Decision Tree) MSE: {mse_dt:.4f}\")\n",
        "print(f\"Bagging Regressor (KNeighbors) MSE: {mse_knn:.4f}\")"
      ],
      "metadata": {
        "id": "D0hPqZGeruq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#22. Train a Random Forest Classifier and evaluate its performance using ROC-AUC Score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_scores = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate ROC-AUC Score\n",
        "roc_auc = roc_auc_score(y_test, y_scores)\n",
        "print(f\"Random Forest ROC-AUC Score: {roc_auc:.4f}\")"
      ],
      "metadata": {
        "id": "pLp3aFNur7OA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#23. Train a Bagging Classifier and evaluate its performance using cross-validation\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Initialize the Bagging Classifier\n",
        "bagging_classifier = BaggingClassifier(DecisionTreeClassifier(), n_estimators=50, random_state=42)\n",
        "\n",
        "# Perform cross-validation\n",
        "cv_scores = cross_val_score(bagging_classifier, X, y, cv=5)\n",
        "\n",
        "print(f\"Cross-Validation Scores: {cv_scores}\")\n",
        "print(f\"Mean Cross-Validation Score: {cv_scores.mean():.4f}\")"
      ],
      "metadata": {
        "id": "B5ZAyPzwsErG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#24. Train a Random Forest Classifier and plot the Precision-Recall curve\n",
        "from sklearn.metrics import precision_recall_curve\n",
        "\n",
        "# Train the Random Forest Classifier\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_scores = rf_classifier.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate precision and recall\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
        "\n",
        "# Plot Precision-Recall curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, marker='.')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve for Random Forest Classifier')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VpfZJ6LXsNZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#25.Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define the base models\n",
        "base_models = [\n",
        "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
        "    ('lr', LogisticRegression())\n",
        "]\n",
        "\n",
        "# Initialize the Stacking Classifier\n",
        "stacking_classifier = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression())\n",
        "\n",
        "# Train the Stacking Classifier\n",
        "stacking_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_stacking = stacking_classifier.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_stacking = accuracy_score(y_test, y_pred_stacking)\n",
        "print(f\"Stacking Classifier Accuracy: {accuracy_stacking:.4f}\")"
      ],
      "metadata": {
        "id": "9dmSZcW0sUwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#26.Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n",
        "# Initialize lists to store MSE results\n",
        "bootstrap_samples = [0.5, 0.7, 0.9]\n",
        "mse_bootstrap_results = []\n",
        "\n",
        "for bootstrap in bootstrap_samples:\n",
        "    bagging_regressor = BaggingRegressor(DecisionTreeRegressor(), n_estimators=50, bootstrap=True, max_samples=bootstrap, random_state=42)\n",
        "    bagging_regressor.fit(X_train_reg, y_train_reg)\n",
        "    y_pred = bagging_regressor.predict(X_test_reg)\n",
        "    mse = mean_squared_error(y_test_reg, y_pred)\n",
        "    mse_bootstrap_results.append(mse)\n",
        "    print(f\"Bagging Regressor with {bootstrap*100:.0f}% bootstrap samples MSE: {mse:.4f}\")"
      ],
      "metadata": {
        "id": "ITtT8zwgsgiF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}